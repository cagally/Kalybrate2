{
  "task_id": "hook-development_medium_3",
  "prompt": "I want to validate that Claude has properly tested any code changes before considering a task complete. Set up hooks that check for test execution and build success.",
  "difficulty": "medium",
  "model": "claude-3-5-haiku-20241022",
  "response": "I'll help you create hooks that validate code changes by checking for test execution and build success. I'll create a comprehensive hook setup that can work across different project types.\n\nLet's create the hooks configuration and supporting scripts:\n\n1. First, I'll create the `hooks.json` file:\n\n```python\nimport os\nimport json\n\nhooks_config = {\n    \"description\": \"Validate code changes with test and build checks\",\n    \"hooks\": {\n        \"Stop\": [\n            {\n                \"matcher\": \"*\",\n                \"hooks\": [\n                    {\n                        \"type\": \"prompt\",\n                        \"prompt\": \"Verify that tests have been run and build succeeded. Check for test coverage, no failing tests, and successful build. Return 'approve' or provide reasons to block stopping.\"\n                    }\n                ]\n            }\n        ],\n        \"PreToolUse\": [\n            {\n                \"matcher\": \"Write|Edit\",\n                \"hooks\": [\n                    {\n                        \"type\": \"command\", \n                        \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/validate-changes.sh\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n\n# Ensure the hooks directory exists\nos.makedirs(os.path.join(OUTPUT_DIR, 'hooks'), exist_ok=True)\n\n# Write hooks.json\nwith open(os.path.join(OUTPUT_DIR, 'hooks', 'hooks.json'), 'w') as f:\n    json.dump(hooks_config, f, indent=2)\n```\n\n2. Now I'll create a validation script for checking code changes:\n\n```python\nimport os\n\nvalidate_script = '''#!/bin/bash\nset -euo pipefail\n\n# Input validation script to check code changes\ninput=$(cat)\n\n# Detect project type\nproject_type=\"\"\nif [ -f \"package.json\" ]; then\n    project_type=\"nodejs\"\nelif [ -f \"pyproject.toml\" ] || [ -f \"setup.py\" ]; then\n    project_type=\"python\"\nelif [ -f \"pom.xml\" ]; then\n    project_type=\"java\"\nelif [ -f \"go.mod\" ]; then\n    project_type=\"go\"\nelse\n    # Unknown project type, skip specific checks\n    exit 0\nfi\n\n# Project-specific validation functions\nvalidate_nodejs() {\n    # Check for test scripts in package.json\n    if ! jq -e '.scripts.test' package.json > /dev/null 2>&1; then\n        echo '{\"decision\": \"ask\", \"reason\": \"No test script found in package.json\"}' >&2\n        exit 2\n    fi\n    \n    # Run tests\n    npm test || {\n        echo '{\"decision\": \"deny\", \"reason\": \"Tests failed\"}' >&2\n        exit 2\n    }\n}\n\nvalidate_python() {\n    # Check for pytest or unittest\n    if [ -f \"pyproject.toml\" ]; then\n        poetry run pytest || {\n            echo '{\"decision\": \"deny\", \"reason\": \"Python tests failed\"}' >&2\n            exit 2\n        }\n    elif [ -f \"setup.py\" ]; then\n        python -m pytest || {\n            echo '{\"decision\": \"deny\", \"reason\": \"Python tests failed\"}' >&2\n            exit 2\n        }\n    fi\n}\n\nvalidate_java() {\n    # Maven tests\n    mvn test || {\n        echo '{\"decision\": \"deny\", \"reason\": \"Java tests failed\"}' >&2\n        exit 2\n    }\n}\n\nvalidate_go() {\n    # Go tests\n    go test ./... || {\n        echo '{\"decision\": \"deny\", \"reason\": \"Go tests failed\"}' >&2\n        exit 2\n    }\n}\n\n# Run project-specific validation\ncase \"$project_type\" in\n    \"nodejs\")\n        validate_nodejs\n        ;;\n    \"python\")\n        validate_python\n        ;;\n    \"java\")\n        validate_java\n        ;;\n    \"go\")\n        validate_go\n        ;;\nesac\n\n# If we get here, changes look good\necho '{\"decision\": \"approve\", \"reason\": \"Changes validated successfully\"}' >&1\nexit 0\n'''\n\n# Ensure the scripts directory exists\nos.makedirs(os.path.join(OUTPUT_DIR, 'hooks', 'scripts'), exist_ok=True)\n\n# Write the validation script\nwith open(os.path.join(OUTPUT_DIR, 'hooks', 'scripts', 'validate-changes.sh'), 'w') as f:\n    f.write(validate_script)\n\n# Make the script executable\nos.chmod(os.path.join(OUTPUT_DIR, 'hooks', 'scripts', 'validate-changes.sh'), 0o755)\n\nprint(f\"Hooks configuration and validation script created in {OUTPUT_DIR}\")\n```\n\n3. I'll also create a README to explain the hooks:\n\n```python\nreadme_content = '''# Code Validation Hooks\n\n## Overview\nThese hooks provide comprehensive validation for code changes across different project types.\n\n## Supported Project Types\n- Node.js (via package.json)\n- Python (via pyproject.toml or setup.py)\n- Java (via pom.xml)\n- Go (via go.mod)\n\n## Validation Checks\n- Verify test scripts exist\n- Run project-specific tests\n- Prevent stopping if tests fail\n\n## Configuration\nHooks are configured to:\n1. Check for test scripts before code changes\n2. Validate tests before task completion\n\n## Requirements\n- jq (for JSON parsing)\n- Respective language test frameworks\n\n## Customization\nModify `validate-changes.sh` to add project-specific validation logic.\n'''\n\n# Write README\nwith open(os.path.join(OUTPUT_DIR, 'hooks', 'README.md'), 'w') as f:\n    f.write(readme_content)\n\nprint(f\"README created in {OUTPUT_DIR}/hooks\")\n```\n\nThis solution provides a flexible, multi-language approach to validating code changes:\n\n1. `hooks.json`: Defines hook configuration for Stop and PreToolUse events\n2. `validate-changes.sh`: A bash script that:\n   - Detects project type\n   - Runs appropriate test suite\n   - Provides detailed validation feedback\n3. README explaining the hook's purpose and usage\n\nThe hooks will:\n- Check for test scripts before making changes\n- Run tests before considering a task complete\n- Block completion if tests fail\n- Support multiple project types\n\nKey features:\n- Adaptive to different project structures\n- Prevents untested code from being considered complete\n- Provides clear feedback on validation failures\n\nWould you like me to explain any part of the implementation or make any modifications?",
  "criteria_results": {
    "file_created": false,
    "file_valid": false,
    "file_has_content": false
  },
  "verification_notes": {
    "file_created": "verified",
    "file_valid": "verified - no file created",
    "file_has_content": "verified - no file created"
  },
  "verification_level": "full",
  "verified_criteria": {
    "passed": 0,
    "total": 1
  },
  "passed": false,
  "tokens": {
    "input": 4844,
    "output": 1595,
    "total": 6439
  },
  "execution_time": 25.594556093215942,
  "error": null,
  "timestamp": "2026-01-01T18:54:38.191496"
}