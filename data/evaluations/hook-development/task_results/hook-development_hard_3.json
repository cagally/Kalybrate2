{
  "task_id": "hook-development_hard_3",
  "prompt": "Design a multi-layered hook system that intercepts user prompts to add security warnings, validates all tool calls with context-aware AI reasoning, provides intelligent post-action feedback, and implements a sophisticated completion gate that ensures code quality, security, and documentation standards are met.",
  "difficulty": "hard",
  "model": "claude-3-5-haiku-20241022",
  "response": "I'll design a comprehensive multi-layered hook system that demonstrates advanced security, validation, and quality control using Claude Code's hook system. I'll create several files to implement this robust hook framework.\n\nLet's start by creating the plugin structure:\n\n1. First, I'll create the main `hooks.json`:\n\n```python\nimport os\nimport json\n\nhooks_config = {\n    \"description\": \"Comprehensive Security and Quality Assurance Hooks\",\n    \"hooks\": {\n        \"UserPromptSubmit\": [\n            {\n                \"matcher\": \"*\",\n                \"hooks\": [\n                    {\n                        \"type\": \"prompt\",\n                        \"prompt\": \"Analyze the user prompt for potential security risks, sensitive information disclosure, or inappropriate content. Provide a security assessment and warning if necessary.\"\n                    }\n                ]\n            }\n        ],\n        \"PreToolUse\": [\n            {\n                \"matcher\": \"*\",\n                \"hooks\": [\n                    {\n                        \"type\": \"prompt\", \n                        \"prompt\": \"Perform a comprehensive context-aware security and appropriateness validation for the upcoming tool use. Assess potential risks, validate input safety, and determine if the tool call should be allowed, modified, or blocked.\"\n                    }\n                ]\n            }\n        ],\n        \"PostToolUse\": [\n            {\n                \"matcher\": \"*\", \n                \"hooks\": [\n                    {\n                        \"type\": \"prompt\",\n                        \"prompt\": \"Analyze the results of the recent tool use. Provide intelligent feedback on code quality, potential improvements, security considerations, and best practices.\"\n                    }\n                ]\n            }\n        ],\n        \"Stop\": [\n            {\n                \"matcher\": \"*\",\n                \"hooks\": [\n                    {\n                        \"type\": \"prompt\", \n                        \"prompt\": \"Conduct a comprehensive quality gate assessment. Verify that all code meets security standards, documentation requirements, and follows best practices. Provide a detailed report and determine if the task can be considered complete.\"\n                    }\n                ]\n            }\n        ],\n        \"SessionStart\": [\n            {\n                \"matcher\": \"*\",\n                \"hooks\": [\n                    {\n                        \"type\": \"command\",\n                        \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/initialize-security-context.sh\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n\n# Write hooks.json to the specified output directory\noutput_dir = os.environ.get('OUTPUT_DIR', '/tmp')\nhooks_path = os.path.join(output_dir, 'hooks.json')\n\nwith open(hooks_path, 'w') as f:\n    json.dump(hooks_config, f, indent=2)\n\nprint(f\"Hooks configuration saved to {hooks_path}\")\n```\n\n2. Next, I'll create an initialization script for security context:\n\n```python\nimport os\n\ninit_script = '''#!/bin/bash\n# Initialize security context and prepare environment for secure development\n\n# Create security configuration directory\nmkdir -p \"${CLAUDE_PROJECT_DIR}/.claude/security\"\n\n# Generate initial security profile\necho \"{\n    \\\"security_level\\\": \\\"high\\\",\n    \\\"approved_tools\\\": [\\\"Write\\\", \\\"Read\\\", \\\"Edit\\\", \\\"Bash\\\"],\n    \\\"blocked_paths\\\": [\n        \\\"**/secrets/**\\\",\n        \\\"**/*.env\\\",\n        \\\"**/*credentials*\\\",\n        \\\"**/private_keys/**\\\"\n    ],\n    \\\"max_file_size_mb\\\": 10,\n    \\\"allowed_file_extensions\\\": [\n        \\\".py\\\", \\\".md\\\", \\\".txt\\\", \\\".json\\\", \\\".yaml\\\", \\\".yml\\\", \\\".sh\\\"\n    ]\n}\" > \"${CLAUDE_PROJECT_DIR}/.claude/security/profile.json\"\n\n# Log initialization\necho \"Security context initialized at $(date)\"\n'''\n\noutput_dir = os.environ.get('OUTPUT_DIR', '/tmp')\nscript_path = os.path.join(output_dir, 'initialize-security-context.sh')\n\nwith open(script_path, 'w') as f:\n    f.write(init_script)\n\n# Make the script executable\nos.chmod(script_path, 0o755)\n\nprint(f\"Security initialization script saved to {script_path}\")\n```\n\n3. Create a README for the plugin:\n\n```python\nimport os\n\nreadme_content = '''# Advanced Security and Quality Assurance Plugin\n\n## Overview\nThis plugin provides a multi-layered security and quality control system for Claude Code, implementing comprehensive hooks to:\n- Validate user prompts\n- Secure tool usage\n- Provide intelligent feedback\n- Enforce code quality standards\n\n## Hook Layers\n1. **User Prompt Security**: Scans incoming prompts for potential risks\n2. **Pre-Tool Validation**: Context-aware security checks before tool execution\n3. **Post-Tool Feedback**: Intelligent analysis of tool results\n4. **Completion Gate**: Final quality and security assessment\n\n## Security Features\n- Prompt risk assessment\n- Tool call validation\n- Path and file restrictions\n- Size and extension limitations\n\n## Configuration\nAdjust security settings in `.claude/security/profile.json`\n\n## Best Practices\n- Always review hook outputs\n- Customize security profile as needed\n- Monitor and update security configurations\n\n## Requirements\n- Claude Code\n- Bash\n- jq (for JSON processing)\n'''\n\noutput_dir = os.environ.get('OUTPUT_DIR', '/tmp')\nreadme_path = os.path.join(output_dir, 'README.md')\n\nwith open(readme_path, 'w') as f:\n    f.write(readme_content)\n\nprint(f\"README saved to {readme_path}\")\n```\n\nThese scripts create a comprehensive hook system with:\n1. Prompt security scanning\n2. Pre-tool validation\n3. Post-tool intelligent feedback\n4. Completion quality gates\n5. Security context initialization\n6. Detailed documentation\n\nThe hook system will:\n- Intercept and analyze user prompts\n- Validate tool calls with AI reasoning\n- Provide context-aware feedback\n- Enforce security and quality standards\n- Allow flexible configuration\n\nKey features include:\n- Dynamic security profiling\n- Comprehensive validation layers\n- Intelligent, context-aware decision making\n- Flexible configuration options\n\nWould you like me to elaborate on any part of the implementation or explain the design philosophy behind this multi-layered hook system?",
  "criteria_results": {
    "file_created": false,
    "file_valid": false,
    "file_has_content": false
  },
  "verification_notes": {
    "file_created": "verified",
    "file_valid": "verified - no file created",
    "file_has_content": "verified - no file created"
  },
  "verification_level": "full",
  "verified_criteria": {
    "passed": 0,
    "total": 1
  },
  "passed": false,
  "tokens": {
    "input": 4870,
    "output": 1505,
    "total": 6375
  },
  "execution_time": 26.56334900856018,
  "error": null,
  "timestamp": "2026-01-01T18:54:38.192117"
}