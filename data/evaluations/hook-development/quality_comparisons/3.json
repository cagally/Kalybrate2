{
  "prompt": "Create hooks that ensure proper testing is done before task completion",
  "baseline_response": "Here's a comprehensive approach to creating hooks for ensuring proper testing before task completion:\n\n```python\nimport functools\nimport logging\nfrom typing import Callable, Any, Dict\nimport pytest\nimport unittest\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass TestHooks:\n    \"\"\"\n    A class providing decorators and hooks for testing and validation\n    \"\"\"\n\n    @staticmethod\n    def pre_test_hook(test_function: Callable = None, \n                      required_tests: Dict[str, Callable] = None):\n        \"\"\"\n        Decorator to run pre-test validation and checks\n        \n        Args:\n            test_function (Callable): Function to be tested\n            required_tests (Dict): Dictionary of test cases to run\n        \n        Returns:\n            Wrapped function with pre-test validation\n        \"\"\"\n        def decorator(func):\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                # Run pre-test validation\n                logger.info(f\"Running pre-test validation for {func.__name__}\")\n                \n                # Execute required tests\n                if required_tests:\n                    for test_name, test_case in required_tests.items():\n                        try:\n                            result = test_case(*args, **kwargs)\n                            if not result:\n                                raise ValueError(f\"Test {test_name} failed\")\n                            logger.info(f\"Test {test_name} passed successfully\")\n                        except Exception as e:\n                            logger.error(f\"Pre-test validation failed: {e}\")\n                            raise\n                \n                # Execute original function\n                return func(*args, **kwargs)\n            return wrapper\n        \n        # Allow decorator to be used with or without parentheses\n        if test_function:\n            return decorator(test_function)\n        return decorator\n\n    @staticmethod\n    def post_test_hook(validator: Callable = None):\n        \"\"\"\n        Decorator to run post-test validation\n        \n        Args:\n            validator (Callable): Validation function to run after task\n        \n        Returns:\n            Wrapped function with post-test validation\n        \"\"\"\n        def decorator(func):\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                # Execute original function\n                result = func(*args, **kwargs)\n                \n                # Run post-test validation\n                logger.info(f\"Running post-test validation for {func.__name__}\")\n                \n                if validator:\n                    try:\n                        validation_result = validator(result)\n                        if not validation_result:\n                            raise ValueError(\"Post-test validation failed\")\n                        logger.info(\"Post-test validation successful\")\n                    except Exception as e:\n                        logger.error(f\"Post-test validation failed: {e}\")\n                        raise\n                \n                return result\n            return wrapper\n        \n        # Allow decorator to be used with or without parentheses\n        if validator:\n            return decorator(validator)\n        return decorator\n\n    @staticmethod\n    def comprehensive_test_hook(\n        pre_tests: Dict[str, Callable] = None, \n        post_validator: Callable = None\n    ):\n        \"\"\"\n        Comprehensive test hook combining pre and post-test validations\n        \n        Args:\n            pre_tests (Dict): Dictionary of pre-test cases\n            post_validator (Callable): Post-test validation function\n        \n        Returns:\n            Decorator for comprehensive testing\n        \"\"\"\n        def decorator(func):\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                # Apply pre-test hook\n                pre_test_decorated = TestHooks.pre_test_hook(\n                    required_tests=pre_tests\n                )(func)\n                \n                # Apply post-test hook\n                post_test_decorated = TestHooks.post_test_hook(\n                    validator=post_validator\n                )(pre_test_decorated)\n                \n                return post_test_decorated(*args, **kwargs)\n            return wrapper\n        return decorator\n\n# Example usage demonstrating various hook scenarios\nclass ExampleTests:\n    @TestHooks.pre_test_hook(\n        required_tests={\n            'input_validation': lambda x: x > 0,\n            'type_check': lambda x: isinstance(x, int)\n        }\n    )\n    def process_number(number):\n        return number * 2\n\n    @TestHooks.post_test_hook(\n        validator=lambda result: result > 0\n    )\n    def complex_calculation(x, y):\n        return x / y\n\n    @TestHooks.comprehensive_test_hook(\n        pre_tests={\n            'input_validation': lambda x: x > 10\n        },\n        post_validator=lambda result: result < 100\n    )\n    def advanced_processing(value):\n        return value * 2\n\n# Demonstration and testing\ndef main():\n    # Test pre-test hook\n    try:\n        result = ExampleTests.process_number(5)\n        print(f\"Pre-test hook result: {result}\")\n    except ValueError as e:\n        print(f\"Pre-test validation error: {e}\")\n\n    # Test post-test hook\n    try:\n        result = ExampleTests.complex_calculation(10, 2)\n        print(f\"Post-test hook result: {result}\")\n    except ValueError as e:\n        print(f\"Post-test validation error: {e}\")\n\n    # Test comprehensive hook\n    try:\n        result = ExampleTests.advanced_processing(15)\n        print(f\"Comprehensive hook result: {result}\")\n    except ValueError as e:\n        print(f\"Comprehensive validation error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nKey Features of the Test Hooks:\n\n1. `pre_test_hook`:\n   - Runs validation tests before executing the main function\n   - Allows multiple pre-tests to be defined\n   - Logs test results and raises exceptions for failed tests\n\n2. `post_test_hook`:\n   - Validates the result of a function after execution\n   - Provides flexibility in defining custom validation logic\n   - Logs validation results\n\n3. `comprehensive_test_hook`:\n   - Combines pre-test and post-test validations\n   - Offers a unified approach to testing and validation\n\nBenefits:\n- Modular and reusable testing approach\n- Flexible validation strategies\n- Comprehensive logging\n- Easy to integrate into existing code\n\nUsage Guidelines:\n- Define custom test cases as lambda functions or separate methods\n- Use decorators to apply tests to functions\n- Handle exceptions and validate inputs/outputs\n\nRecommendations:\n- Implement detailed logging\n- Use type hints\n- Create specific test cases for different scenarios\n- Consider performance overhead of extensive testing\n\nThis implementation provides a robust framework for implementing testing hooks with flexible validation strategies.",
  "baseline_tokens": {
    "input": 18,
    "output": 1631,
    "total": 1649
  },
  "skill_response": "I'll help you create hooks that enforce proper testing before task completion. We'll implement hooks for both PreToolUse and Stop events to validate testing requirements.\n\nI'll create a comprehensive hook configuration that:\n1. Checks for test existence before allowing code modifications\n2. Ensures tests pass before allowing task completion\n3. Provides flexible, prompt-based validation\n\nHere's a detailed implementation:\n\nFirst, let's create the hooks configuration in `hooks/hooks.json`:\n\n```json\n{\n  \"description\": \"Ensure proper testing is performed before code changes and task completion\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Validate test coverage before file modification:\\n\\nCheck the following:\\n1. Are there existing tests for the module being modified?\\n2. Do the current tests cover the functionality being changed?\\n3. If no tests exist, is a test plan required?\\n\\nEvaluate the test situation and respond with:\\n- 'approve': Sufficient test coverage exists\\n- 'ask': Need more information about tests\\n- 'block': No tests found, tests required\\n\\nProvide a detailed explanation of your decision.\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"Stop\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\", \n            \"prompt\": \"Verify task completion checklist:\\n\\n1. Have all new/modified code components been tested?\\n2. Are all existing tests passing?\\n3. Have you added new tests for recent changes?\\n4. Is test coverage sufficient?\\n5. Have you run comprehensive test suites?\\n\\nEvaluate the testing readiness and respond with:\\n- 'approve': All testing requirements met\\n- 'block': Testing is incomplete\\n\\nIf blocking, provide specific guidance on what testing steps are needed.\",\n            \"timeout\": 45\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nLet's also create some supporting scripts to enhance the hook functionality.\n\nCreate a script at `${CLAUDE_PLUGIN_ROOT}/scripts/check-tests.sh`:\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\n# Function to check for test files\nfind_tests() {\n    local dir=\"$1\"\n    local test_patterns=(\n        \"*test*.js\" \"*spec*.js\"  # JavaScript\n        \"*test*.py\" \"*spec*.py\"  # Python\n        \"*Test*.java\"            # Java\n        \"*_test.go\"              # Go\n        \"*_spec.rb\"              # Ruby\n    )\n    \n    local tests_found=0\n    for pattern in \"${test_patterns[@]}\"; do\n        if find \"$dir\" -name \"$pattern\" | grep -q .; then\n            tests_found=1\n            break\n        fi\n    done\n    \n    return $tests_found\n}\n\n# Read input from stdin\ninput=$(cat)\n\n# Extract current working directory\ncwd=$(echo \"$input\" | jq -r '.cwd // \".\"')\n\n# Check for test files\nif find_tests \"$cwd\"; then\n    echo '{\n        \"decision\": \"approve\",\n        \"systemMessage\": \"Test files found. Proceeding with validation.\",\n        \"details\": {\n            \"testStatus\": \"present\",\n            \"message\": \"Test files detected in the project directory\"\n        }\n    }'\nelse\n    echo '{\n        \"decision\": \"block\",\n        \"systemMessage\": \"No test files found. Please create tests before proceeding.\",\n        \"details\": {\n            \"testStatus\": \"missing\",\n            \"message\": \"No test files detected. Create tests covering the new/modified code.\"\n        }\n    }' >&2\n    exit 2\nfi\n```\n\nMake the script executable:\n```bash\nchmod +x ${CLAUDE_PLUGIN_ROOT}/scripts/check-tests.sh\n```\n\nCreate a README for the hook configuration:\n\n```markdown\n# Testing Enforcement Hooks\n\n## Overview\nThese hooks ensure that:\n- Code modifications are only allowed with adequate test coverage\n- Tasks are not considered complete without proper testing\n- Test existence and quality are validated before proceeding\n\n## Validation Checks\n- PreToolUse Hook:\n  1. Checks for existing test files before code modification\n  2. Prompts for test coverage assessment\n  3. Blocks modifications without tests\n\n- Stop Hook:\n  1. Verifies comprehensive testing before task completion\n  2. Checks for test existence and passing status\n  3. Ensures new changes are tested\n\n## Requirements\n- Supports multiple programming languages\n- Flexible test file detection\n- Prompt-based and command-based validation\n\n## Recommended Action\n- Always maintain up-to-date test files\n- Cover new code with appropriate tests\n- Run test suites before task completion\n```\n\nAdditional recommendations:\n\n1. **Test Framework Integration**\n   You can extend `check-tests.sh` to run actual test suites:\n   ```bash\n   # Example test running (customize for your project)\n   if npm test || python -m pytest || go test ./...; then\n       # Tests passed\n   else\n       # Tests failed\n   fi\n   ```\n\n2. **Language-Specific Enhancements**\n   - Add language-specific test detection\n   - Integrate with project's native testing frameworks\n   - Support various test runners (Jest, pytest, RSpec, etc.)\n\n3. **Configurable Strictness**\n   Create a `.claude/test-config.json` to allow project-specific test requirements:\n   ```json\n   {\n     \"requiredTestCoverage\": 80,\n     \"mandatoryTestTypes\": [\"unit\", \"integration\"],\n     \"testFrameworks\": [\"jest\", \"pytest\"]\n   }\n   ```\n\n**Deployment Steps:**\n1. Place `hooks/hooks.json` in your project\n2. Add `scripts/check-tests.sh`\n3. Ensure `jq` is installed for JSON processing\n4. Restart Claude Code session\n\nThis implementation provides a robust, flexible approach to ensuring proper testing before code modifications and task completion. The hooks offer both prompt-based and command-based validation, supporting various programming languages and project structures.\n\nWould you like me to elaborate on any part of the testing enforcement hook configuration?",
  "skill_tokens": {
    "input": 4724,
    "output": 1489,
    "total": 6213
  },
  "judge_verdict": "with_skill",
  "judge_reasoning": "Response A directly addresses the task by creating hooks that enforce testing requirements before task completion, using a JSON configuration format that appears designed for a specific hook system. It provides comprehensive validation at both pre-tool-use and task completion stages. Response B creates Python decorators for testing, which is useful but doesn't actually create 'hooks that ensure proper testing is done before task completion' - it's more of a general testing framework. Response A is more targeted, complete, and directly solves the stated problem.",
  "judge_model": "claude-sonnet-4-20250514",
  "timestamp": "2026-01-01T18:59:04.683714"
}